{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders: a feature representation technique\n",
    "\n",
    "## Introduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore how to design an autoencoder class and apply it for features representation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first make sure you have [training](https://d396qusza40orc.cloudfront.net/phoenixassets/course1-for-students/image_test_data.zip) and [testing](https://d396qusza40orc.cloudfront.net/phoenixassets/course1-for-students/image_train_data.zip) data. It's a CIFAR-10 dataset but with only 4 classes, courtesy [Prof. Carlos](https://www.coursera.org/learn/ml-foundations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Autoencoders](https://github.com/HFTrader/DeepLearningBook/blob/master/files/Chap14.pdf) are a special type for neural networks where the architecture is required to learn the input itself i.e. \n",
    "\n",
    "input --> (deepNeuralNetwork) --> input\n",
    "\n",
    "One might question, why waste resources in learning hidden layers; instead just map a constant linear function $$f:R^n => R^n$$\n",
    "\n",
    "But the architecture can be made such that the hidden layers can have lower dimensionality to \"represent\" an input. So autoencoders can be used as a technique to reduce dimensions and since it's NN architecture, it can reduce dimensionality *non-linearly* unlike PCA which is a linear dimensionality reduction technique. For a thorough review, learn more at [github](https://github.com/HFTrader/DeepLearningBook/blob/master/files/Chap14.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting-up the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First: Write a simple AutoEncoder class using Tensorflow\n",
    "* Second: Import all the required packages and datasets\n",
    "* Third: Train the autoencoder \n",
    "* Fourth: Autoencode the input and run a simple linear classifier\n",
    "* Fifth: Wrap up and notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write a simplest AutoEncoder class using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(object):\n",
    "    \"\"\"\n",
    "    A deep learning autoencoder class implemented using TensorFlow\n",
    "    \n",
    "    https://github.com/nonlocal\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape = (None,3072),  hidden_layers = 1, hidden_units = [1024,512,256]):\n",
    "        \"\"\"\n",
    "        Initialize the class:\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            input_shape : tuple, of the form (None, int)\n",
    "                The second element in the tuple is the number of \"features\" in a sample input\n",
    "            hidden_units: list, int elements\n",
    "                The number of hidden units in the \"encoding\" layers\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "\n",
    "        \"\"\"\n",
    "        _, self.n_inputs = input_shape\n",
    "        self._new_units =list([self.n_inputs]+hidden_units)\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        self._n_hidden = len(hidden_units)\n",
    "        \n",
    "        for i in range(len(self._new_units)-1):\n",
    "            self.weights[\"W_encode\"+\"{:d}\".format(i+1)] = tf.Variable(tf.random_normal([self._new_units[i],self._new_units[i+1]]),name=\"W_encode\"+\"{:d}\".format(i+1))\n",
    "            self.biases[\"B_encode\"+\"{:d}\".format(i+1)] = tf.Variable(tf.random_normal([self._new_units[i+1]]), name=\"B_encode\"+\"{:d}\".format(i+1))\n",
    "            self.weights[\"W_decode\"+\"{:d}\".format(len(self._new_units)-1-i)] = tf.Variable(tf.random_normal([self._new_units[-(i+1)],self._new_units[-(i+2)]]),name=\"W_decode\"+\"{:d}\".format(len(self._new_units)-1-i))\n",
    "            self.biases[\"B_decode\"+\"{:d}\".format(i+1)] = tf.Variable(tf.random_normal([self._new_units[i]]), name=\"B_decode\"+\"{:d}\".format(len(self._new_units)-1-i))         \n",
    "    \n",
    "    def encode(self, arr):\n",
    "        \"\"\"\n",
    "        Encode the given input sample(s)\n",
    "            Parameters\n",
    "            ----------\n",
    "            arr : array (of array)\n",
    "                The input to encode\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            code: array (of array)\n",
    "                The \"code\" for the given input\n",
    "        \"\"\"\n",
    "        code = arr\n",
    "        for i in range(len(self._new_units)-1):\n",
    "            code = tf.nn.tanh(tf.add(tf.matmul(code, self.weights[i]), self.biases[i]))\n",
    "        return code\n",
    "    \n",
    "    def decode(self, code):\n",
    "        \"\"\"\n",
    "        Decode the given code(s)\n",
    "            Parameters\n",
    "            ----------\n",
    "            code : array (of array)\n",
    "                The code to decode\n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            recon: array (of array)\n",
    "                The reconstructed input\n",
    "        \"\"\"\n",
    "        recon = code\n",
    "        for i in range(len(self._new_units)-1):\n",
    "            recon = tf.nn.tanh(tf.add(tf.matmul(recon, tf.transpose(self.weights[i])), self.biases[self._n_hidden+i]))\n",
    "        return recon\n",
    "    def train(self, arr, batch_size=50, epochs=10, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Train the autoencoder.\n",
    "        The error function is MSE, trained with AdamOptimizer\n",
    "            Parameters\n",
    "            ----------\n",
    "            arr : array (of array), np.float32 or tf.floar32\n",
    "                The input to autoencode\n",
    "            batch_size : int, optional\n",
    "                The number of batches to train at a time\n",
    "            epochs : int, optional\n",
    "                The epochs to make on the dataset\n",
    "            learning_rate : int, optional\n",
    "                The learning rate for Adam Optimizer            \n",
    "                \n",
    "            Returns\n",
    "            -------\n",
    "            \n",
    "            Note\n",
    "            ----\n",
    "            MSE function is not recommended. It tries to blurr the images.\n",
    "            \n",
    "        \"\"\"\n",
    "        import time\n",
    "        INPUT_SIZE = self.n_inputs\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, INPUT_SIZE])\n",
    "        code = self.encode(x)\n",
    "        y = self.decode(code)\n",
    "        error = tf.reduce_sum(tf.square(y - x))\n",
    "        optimization = tf.train.AdamOptimizer(learning_rate).minimize(error)\n",
    "        \n",
    "        n_samp,_ = arr.shape\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            start = time.time()\n",
    "            for j in range(n_samp//batch_size):\n",
    "                sample = np.arange(j*batch_size, j*batch_size+batch_size)\n",
    "                batch_xs = arr[sample]\n",
    "                sess.run(train_step, feed_dict={x: batch_xs})\n",
    "            print (\"Time: %s, \\t Epochs completed: %s, \\t MSE: %s\") %(time.time()- start, i+1, sess.run(cost, feed_dict={x: batch_xs}))\n",
    "        print(\"Final parameters:\")\n",
    "        print (\"Time: %f, \\t Epochs completed: %s, \\t MSE: %s\") %(time.time()- start, i+1, sess.run(cost, feed_dict={x: batch_xs - MEAN_IMG}))\n",
    "        return \"Model Trained.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(AutoEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class demands some definition. Let's go over every function:\n",
    "* init :\n",
    "    This function create a neural network with 3 encoding layers and hence 3 decoding layers + the input layer. More specifically, here's the architeture(#hidden units):\n",
    "\n",
    "    * layers structure:\n",
    "    \n",
    "    input-> layer1-> layer2-> layer3->layer4->layer5->layer6\n",
    "\n",
    "    * layer functions:\n",
    "    \n",
    "    input-> encode-> encode-> encode-> decode -> decode -> decode\n",
    "\n",
    "    * #units units per layers:\n",
    "    \n",
    "    3072 -> 1024 -> 512 -> 256 -> 512 -> 1024 -> 3072\n",
    "    \n",
    "The weights are defined such that a decoding layer has weights transpose that of corresponding encoding layer i.e. symmetric weights or \"tied\" weights\n",
    "\n",
    "\n",
    "* decode:\n",
    "\n",
    "    For decoding layers we use tanh function as activation for neural units.\n",
    "    \n",
    "\n",
    "* encode:\n",
    "\n",
    "    For encoding layers, the activation function is tanh\n",
    "    \n",
    "\n",
    "* train:\n",
    "\n",
    "    To use encoding and decoding functions of autoencoder, we first need to train it: here it's trained with MSE as cost and Adam Optimizer as training algorithm. Other error functions are advised for high constrast and resolution; MSE tried to average the neighbourhood of a region hence not recommended for this purpose; cross-entropy is a good choice. It is HIGHLY recommended to train the model usin Adam optimization. Maybe I am wrong, but, so far in my experience, in every step, the error is observed to decrease monotonically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the required packages and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import graphlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the sizes and names incorrect. Make sure train dataset has more samples\n",
    "image_train = graphlab.SFrame(\"image_test_data/\")\n",
    "image_test = graphlab.SFrame(\"image_train_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_test.shape, image_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SFrame has 5 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_train.column_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id: the ID of the image,\n",
    "\n",
    "image: the actual image of the sample,\n",
    "\n",
    "label: what's label for the sample, {cat, dog, automobile, bird}\n",
    "\n",
    "deep_features: features extracted from AlexNet\n",
    "\n",
    "image_array: raw pixel values of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert the SFrame to pandas.DataFrame\n",
    "image_train = image_train.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_train.drop(['id', 'image'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae = AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_train.image_array = image_train.image_array.apply(lambda x: np.array(x, np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = ae.train(image_train.image_array.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a long long time!! The number of parameters to be learned is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((3072*1024) + 1024 +(1024*512) + 512 + (512*256) +256)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.6 million!! Brace yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#en\"code\" the raw image pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_train['ae_features'] = image_train.image_array.apply(lambda x: sess.run( ae.encode(np.array(x, dtype=np.float32).reshape(-1, 3072))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a simple classifier to train with 'ae_features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the dataframe back to SFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_train = graphlab.SFrame(image_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(image_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_feature_model = graphlab.logistic_classifier.create(image_train,\n",
    "                                                         features=['ae_feature'],\n",
    "                                                         target='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up:\n",
    "\n",
    "One interesting thing observed was that the Adam Optimizer was *always* reducing the error.I have know idea why. Have to check it out. Plus it takes a long long long time to learn 7.6 million parameters, so it's better to keep it running at night with smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
